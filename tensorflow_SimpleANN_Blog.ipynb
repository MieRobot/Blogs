{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Tensorflow Addition to test that all is ok "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5.   4.  20.]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# Turn off TensorFlow warning messages in program output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Define computational graph\n",
    "X = tf.placeholder(tf.float32, name=\"X\")\n",
    "Y = tf.placeholder(tf.float32, name=\"Y\")\n",
    "\n",
    "addition = tf.add(X, Y, name=\"addition\")\n",
    "\n",
    "\n",
    "# Create the session\n",
    "with tf.Session() as session:\n",
    "\n",
    "    result = session.run(addition, feed_dict={X: [1, 2, 10], Y: [4, 2, 10]})\n",
    "\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale input values using this code block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400, 9)\n",
      "(400, 1)\n",
      "Note: Y values were scaled by multiplying by 0.0000036968 and adding -0.1159\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "print(X_scaled_testing.shape)\n",
    "print(Y_scaled_testing.shape)\n",
    "\n",
    "print(\"Note: Y values were scaled by multiplying by {:.10f} and adding {:.4f}\".format(Y_scaler.scale_[0], Y_scaler.min_[0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training pass: 0\n",
      "Training pass: 1\n",
      "Training pass: 2\n",
      "Training pass: 3\n",
      "Training pass: 4\n",
      "Training pass: 5\n",
      "Training pass: 6\n",
      "Training pass: 7\n",
      "Training pass: 8\n",
      "Training pass: 9\n",
      "Training pass: 10\n",
      "Training pass: 11\n",
      "Training pass: 12\n",
      "Training pass: 13\n",
      "Training pass: 14\n",
      "Training pass: 15\n",
      "Training pass: 16\n",
      "Training pass: 17\n",
      "Training pass: 18\n",
      "Training pass: 19\n",
      "Training pass: 20\n",
      "Training pass: 21\n",
      "Training pass: 22\n",
      "Training pass: 23\n",
      "Training pass: 24\n",
      "Training pass: 25\n",
      "Training pass: 26\n",
      "Training pass: 27\n",
      "Training pass: 28\n",
      "Training pass: 29\n",
      "Training pass: 30\n",
      "Training pass: 31\n",
      "Training pass: 32\n",
      "Training pass: 33\n",
      "Training pass: 34\n",
      "Training pass: 35\n",
      "Training pass: 36\n",
      "Training pass: 37\n",
      "Training pass: 38\n",
      "Training pass: 39\n",
      "Training pass: 40\n",
      "Training pass: 41\n",
      "Training pass: 42\n",
      "Training pass: 43\n",
      "Training pass: 44\n",
      "Training pass: 45\n",
      "Training pass: 46\n",
      "Training pass: 47\n",
      "Training pass: 48\n",
      "Training pass: 49\n",
      "Training pass: 50\n",
      "Training pass: 51\n",
      "Training pass: 52\n",
      "Training pass: 53\n",
      "Training pass: 54\n",
      "Training pass: 55\n",
      "Training pass: 56\n",
      "Training pass: 57\n",
      "Training pass: 58\n",
      "Training pass: 59\n",
      "Training pass: 60\n",
      "Training pass: 61\n",
      "Training pass: 62\n",
      "Training pass: 63\n",
      "Training pass: 64\n",
      "Training pass: 65\n",
      "Training pass: 66\n",
      "Training pass: 67\n",
      "Training pass: 68\n",
      "Training pass: 69\n",
      "Training pass: 70\n",
      "Training pass: 71\n",
      "Training pass: 72\n",
      "Training pass: 73\n",
      "Training pass: 74\n",
      "Training pass: 75\n",
      "Training pass: 76\n",
      "Training pass: 77\n",
      "Training pass: 78\n",
      "Training pass: 79\n",
      "Training pass: 80\n",
      "Training pass: 81\n",
      "Training pass: 82\n",
      "Training pass: 83\n",
      "Training pass: 84\n",
      "Training pass: 85\n",
      "Training pass: 86\n",
      "Training pass: 87\n",
      "Training pass: 88\n",
      "Training pass: 89\n",
      "Training pass: 90\n",
      "Training pass: 91\n",
      "Training pass: 92\n",
      "Training pass: 93\n",
      "Training pass: 94\n",
      "Training pass: 95\n",
      "Training pass: 96\n",
      "Training pass: 97\n",
      "Training pass: 98\n",
      "Training pass: 99\n",
      "Training is complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Turn off TensorFlow warning messages in program output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "\n",
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(\"weights4\", shape=[layer_3_nodes, number_of_outputs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_3_output, weights) + biases\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "\n",
    "        # Print the current training status to the screen\n",
    "        print(\"Training pass: {}\".format(epoch))\n",
    "\n",
    "    # Training is now complete!\n",
    "    print(\"Training is complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the model now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.252328 0.271146\n",
      "5 0.0417129 0.0477419\n",
      "10 0.0277126 0.0286039\n",
      "15 0.0294811 0.0298946\n",
      "20 0.012983 0.014064\n",
      "25 0.0104657 0.0122803\n",
      "30 0.0103703 0.0122971\n",
      "35 0.00700808 0.00830673\n",
      "40 0.00482574 0.00541551\n",
      "45 0.00440371 0.00466756\n",
      "50 0.00311241 0.00330238\n",
      "55 0.00266482 0.00281656\n",
      "60 0.00207663 0.00212619\n",
      "65 0.0016705 0.00166426\n",
      "70 0.00133071 0.00134988\n",
      "75 0.00110381 0.00117823\n",
      "80 0.000923177 0.00101176\n",
      "85 0.000803839 0.000890625\n",
      "90 0.00070036 0.00079724\n",
      "95 0.00063146 0.000731107\n",
      "Training is complete!\n",
      "Final Training cost: 0.0005795947508886456\n",
      "Final Testing cost: 0.0006721512763760984\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Turn off TensorFlow warning messages in program output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "\n",
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(\"weights4\", shape=[layer_3_nodes, number_of_outputs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_3_output, weights) + biases\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "\n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch % 5 == 0:\n",
    "            training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y:Y_scaled_training})\n",
    "            testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y:Y_scaled_testing})\n",
    "\n",
    "            print(epoch, training_cost, testing_cost)\n",
    "\n",
    "    # Training is now complete!\n",
    "    print(\"Training is complete!\")\n",
    "\n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the final Neural network code below# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 - Training Cost: 0.04143104702234268  Testing Cost: 0.047244179993867874\n",
      "Epoch: 5 - Training Cost: 0.022631241008639336  Testing Cost: 0.023864833638072014\n",
      "Epoch: 10 - Training Cost: 0.01235070638358593  Testing Cost: 0.013745627366006374\n",
      "Epoch: 15 - Training Cost: 0.009693569503724575  Testing Cost: 0.010630475357174873\n",
      "Epoch: 20 - Training Cost: 0.0058762445114552975  Testing Cost: 0.00638952711597085\n",
      "Epoch: 25 - Training Cost: 0.003568934043869376  Testing Cost: 0.0038745561614632607\n",
      "Epoch: 30 - Training Cost: 0.002544024493545294  Testing Cost: 0.0027273630257695913\n",
      "Epoch: 35 - Training Cost: 0.001941888127475977  Testing Cost: 0.0020427312701940536\n",
      "Epoch: 40 - Training Cost: 0.0014663097681477666  Testing Cost: 0.0015559822786599398\n",
      "Epoch: 45 - Training Cost: 0.001080115674994886  Testing Cost: 0.0012178990291431546\n",
      "Epoch: 50 - Training Cost: 0.0008451386238448322  Testing Cost: 0.001011720160022378\n",
      "Epoch: 55 - Training Cost: 0.0006750869797542691  Testing Cost: 0.0008214210974983871\n",
      "Epoch: 60 - Training Cost: 0.0005423012771643698  Testing Cost: 0.000712149718310684\n",
      "Epoch: 65 - Training Cost: 0.0004527715500444174  Testing Cost: 0.0006117864977568388\n",
      "Epoch: 70 - Training Cost: 0.00038073002360761166  Testing Cost: 0.0005198200233280659\n",
      "Epoch: 75 - Training Cost: 0.00033073860686272383  Testing Cost: 0.00045570306247100234\n",
      "Epoch: 80 - Training Cost: 0.00029069563606753945  Testing Cost: 0.00040953682037070394\n",
      "Epoch: 85 - Training Cost: 0.00025683731655590236  Testing Cost: 0.00036458243266679347\n",
      "Epoch: 90 - Training Cost: 0.00022870194516144693  Testing Cost: 0.0003308342129457742\n",
      "Epoch: 95 - Training Cost: 0.00020537381351459771  Testing Cost: 0.00030951749067753553\n",
      "Final Training cost: 0.00018961391469929367\n",
      "Final Testing cost: 0.00029461499070748687\n",
      "The actual earnings of Game #1 were $247537.0\n",
      "Our neural network predicted earnings of $253079.109375\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Turn off TensorFlow warning messages in program output\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"sales_data_training.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_training = training_data_df.drop('total_earnings', axis=1).values\n",
    "Y_training = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"sales_data_test.csv\", dtype=float)\n",
    "\n",
    "# Pull out columns for X (data to train with) and Y (value to predict)\n",
    "X_testing = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_testing = test_data_df[['total_earnings']].values\n",
    "\n",
    "# All data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well. Create scalers for the inputs and outputs.\n",
    "X_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "Y_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "X_scaled_training = X_scaler.fit_transform(X_training)\n",
    "Y_scaled_training = Y_scaler.fit_transform(Y_training)\n",
    "\n",
    "# It's very important that the training and test data are scaled with the same scaler.\n",
    "X_scaled_testing = X_scaler.transform(X_testing)\n",
    "Y_scaled_testing = Y_scaler.transform(Y_testing)\n",
    "\n",
    "# Define model parameters\n",
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "\n",
    "# Define how many inputs and outputs are in our neural network\n",
    "number_of_inputs = 9\n",
    "number_of_outputs = 1\n",
    "\n",
    "# Define how many neurons we want in each layer of our neural network\n",
    "layer_1_nodes = 50\n",
    "layer_2_nodes = 100\n",
    "layer_3_nodes = 50\n",
    "\n",
    "# Section One: Define the layers of the neural network itself\n",
    "\n",
    "# Input Layer\n",
    "with tf.variable_scope('input'):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, number_of_inputs))\n",
    "\n",
    "# Layer 1\n",
    "with tf.variable_scope('layer_1'):\n",
    "    weights = tf.get_variable(\"weights1\", shape=[number_of_inputs, layer_1_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases1\", shape=[layer_1_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_1_output = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "\n",
    "# Layer 2\n",
    "with tf.variable_scope('layer_2'):\n",
    "    weights = tf.get_variable(\"weights2\", shape=[layer_1_nodes, layer_2_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases2\", shape=[layer_2_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_2_output = tf.nn.relu(tf.matmul(layer_1_output, weights) + biases)\n",
    "\n",
    "# Layer 3\n",
    "with tf.variable_scope('layer_3'):\n",
    "    weights = tf.get_variable(\"weights3\", shape=[layer_2_nodes, layer_3_nodes], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases3\", shape=[layer_3_nodes], initializer=tf.zeros_initializer())\n",
    "    layer_3_output = tf.nn.relu(tf.matmul(layer_2_output, weights) + biases)\n",
    "\n",
    "# Output Layer\n",
    "with tf.variable_scope('output'):\n",
    "    weights = tf.get_variable(\"weights4\", shape=[layer_3_nodes, number_of_outputs], initializer=tf.contrib.layers.xavier_initializer())\n",
    "    biases = tf.get_variable(name=\"biases4\", shape=[number_of_outputs], initializer=tf.zeros_initializer())\n",
    "    prediction = tf.matmul(layer_3_output, weights) + biases\n",
    "\n",
    "# Section Two: Define the cost function of the neural network that will measure prediction accuracy during training\n",
    "\n",
    "with tf.variable_scope('cost'):\n",
    "    Y = tf.placeholder(tf.float32, shape=(None, 1))\n",
    "    cost = tf.reduce_mean(tf.squared_difference(prediction, Y))\n",
    "\n",
    "# Section Three: Define the optimizer function that will be run to optimize the neural network\n",
    "\n",
    "with tf.variable_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
    "\n",
    "# Create a summary operation to log the progress of the network\n",
    "with tf.variable_scope('logging'):\n",
    "    tf.summary.scalar('current_cost', cost)\n",
    "    summary = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "# Initialize a session so that we can run TensorFlow operations\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # Run the global variable initializer to initialize all variables and layers of the neural network\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create log file writers to record training progress.\n",
    "    # We'll store training and testing log data separately.\n",
    "    training_writer = tf.summary.FileWriter(\"./logs/training\", session.graph)\n",
    "    testing_writer = tf.summary.FileWriter(\"./logs/testing\", session.graph)\n",
    "\n",
    "    # Run the optimizer over and over to train the network.\n",
    "    # One epoch is one full run through the training data set.\n",
    "    for epoch in range(training_epochs):\n",
    "\n",
    "        # Feed in the training data and do one step of neural network training\n",
    "        session.run(optimizer, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "\n",
    "        # Every 5 training steps, log our progress\n",
    "        if epoch % 5 == 0:\n",
    "            # Get the current accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "            training_cost, training_summary = session.run([cost, summary], feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "            testing_cost, testing_summary = session.run([cost, summary], feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
    "\n",
    "            # Write the current training status to the log files (Which we can view with TensorBoard)\n",
    "            training_writer.add_summary(training_summary, epoch)\n",
    "            testing_writer.add_summary(testing_summary, epoch)\n",
    "\n",
    "\n",
    "            # Print the current training status to the screen\n",
    "            print(\"Epoch: {} - Training Cost: {}  Testing Cost: {}\".format(epoch, training_cost, testing_cost))\n",
    "\n",
    "    # Training is now complete!\n",
    "\n",
    "    # Get the final accuracy scores by running the \"cost\" operation on the training and test data sets\n",
    "    final_training_cost = session.run(cost, feed_dict={X: X_scaled_training, Y: Y_scaled_training})\n",
    "    final_testing_cost = session.run(cost, feed_dict={X: X_scaled_testing, Y: Y_scaled_testing})\n",
    "\n",
    "    print(\"Final Training cost: {}\".format(final_training_cost))\n",
    "    print(\"Final Testing cost: {}\".format(final_testing_cost))\n",
    "\n",
    "    # Now that the neural network is trained, let's use it to make predictions for our test data.\n",
    "    # Pass in the X testing data and run the \"prediciton\" operation\n",
    "    Y_predicted_scaled = session.run(prediction, feed_dict={X: X_scaled_testing})\n",
    "\n",
    "    # Unscale the data back to it's original units (dollars)\n",
    "    Y_predicted = Y_scaler.inverse_transform(Y_predicted_scaled)\n",
    "\n",
    "    real_earnings = test_data_df['total_earnings'].values[0]\n",
    "    predicted_earnings = Y_predicted[0][0]\n",
    "\n",
    "    print(\"The actual earnings of Game #1 were ${}\".format(real_earnings))\n",
    "    print(\"Our neural network predicted earnings of ${}\".format(predicted_earnings))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
